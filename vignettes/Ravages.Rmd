---
title: "Package Ravages (RAre Variant Analysis and GEnetic Simulation)"
author: "Herve Perdry and Ozvan Bocher"
date: "`r Sys.Date()`"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r message=FALSE, warning=FALSE}
library("knitr")
require("Ravages")
``` 
## Introduction

Ravages was developped to simulate genetic data and to perform rare variant association tests (burden tests and the variance-component test SKAT) on more than two groups of individuals (Bocher et al., 2019, Genetic Epidemiology). Ravages relies on the package Gaston developped by Herve Perdry and Claire Dandine-Roulland. Most functions are written in C++ thanks to the packages Rcpp, RcppParallel and RcppEigen.  
Functions of Ravages use bed.matrix to manipulate genetic data as in the package Gaston (see documentation of this package for more details).  
In this vignette, we illustrate how to perform rare variant association tests on real data. A second vignette is available showing how to simulate genetic data and how to use them for power calculation.
To learn more about all options of the functions, the reader is advised to look at the manual pages.


##Example of analysis using LCT data
Below is an example of an association analysis and previous steps of data filtering using the dataset LCT available with the package Ravages. This dataset containts data from the 1000Genome project in the region containing the Lactase gene. In this example, we look for an association between rare variants and the populations of 1000Genomes EUR. The population of each individual is available in the dataframe LCT.matrix.pop1000G.
Details about each function is given right after this example.
```{r}
#Importation of data in a bed matrix
x <- as.bed.matrix(x=LCT.matrix.bed, fam=LCT.matrix.fam, bim=LCT.snps)
#Add population
x@ped[,c("pop", "superpop")] <- LCT.matrix.pop1000G[,c("population", "super.population")]

#Select EUR superpopulation
x <- select.inds(x, superpop=="EUR")
x@ped$pop <- droplevels(x@ped$pop)

# Group variants within know genes by extending their positions
# 500bp upstream and downstream
x <- set.genomic.region(x, flank.width=500)
table(x@snps$genomic.region, useNA = "ifany")

# Group variants within known genes using their exact positions
x <- set.genomic.region(x, flank.width=0)
table(x@snps$genomic.region, useNA = "ifany")

# Filter variants with maf in the entire sample lower than 1%
# And keep only genomic region with at least 10 SNPs
x1 <- filter.rare.variants(x, filter = "whole", maf.threshold = 0.01, min.nb.snps = 10)
table(x1@snps$genomic.region, useNA="ifany")

# run burden test CAST, using the 1000Genome population as "outcome"
burden.mlogit(x1, group=x1@ped$pop, burden = "CAST", ref.level = "CEU")

# run SKAT, using the 1000Genome population as "outcome"
SKAT(x1, group=x1@ped$pop)

```

##Defining genomic regions
For rare variant association tests, the unit of analysis is not a single variant but a genomic region, typically a gene. The first step of the analysis is therefore to group variants into genomic regions. This can be done using the function **set.genomic.region()** and known gene positions. It works on a bed.matrix (see Gaston) and simply adds a column "genomic.region" to the slot x@snps containing the gene assigned to each variant. Gene positions should be given to *genes* as a dataframe containing the following columns: *Chr*, *Start*, *End*, *Gene_Name*. By default, any variant being outside the gene positions won't be annotated. Gene positions can be extended to annotate more variants using the argument *flank.width* corresponding to the number of base pairs upstream and downstream the gene. If *flank.width=Inf*, each variant will be assigned to the nearest gene. If two genes overlap, variants in the overlapping zone will be attributed to the second one (in the order given by the position of their starting point on the genome).  
The files **genes.b37** and **genes.b38** available in Ravages which contain gene positions from ENSEMBL versions hg19 and hg38 can be used for *genes* to define gene positions. 


##Rare variant definition
To perform rare variant analysis, it is also important to define what is a rare variant in order to leave out common ones. The function **filter.rare.variants()** enables to keep only variants with a MAF (Minor Allele Frequency) below a given threshold while leaving out monomorphic variants. This function uses and returns a bed.matrix which can be filtered in three different ways:  

* If *filter="whole"*, all the variants with a MAF lower than the threshold in the entire sample will be kept. 
* If *filter="controls"*, all the variants with a MAF lower than the threshold in the controls group will be kept. In this situation, the controls group needs to be specified to the argument *ref.level*.
* If *filter="any"*, all the variants with a MAF lower than the threshold in any of the groups will be kept.  

It is also possible to specify the minimum number of variants needed in a genomic region to keep it using the parameter *min.nb.snps*.


##Rare variant association tests
We have implemented two burden tests extensions (CAST and WSS) and an extension of the variance-component test SKAT to perform the association tests between a gene and more than two groups of individuals.  
The general idea of burden tests is to compute a genetic score per individual and per genomic region and to test if it differs between the different groups of individuals. To extend these tests to more than two groups of individuals, a non-ordinal multinomial regression is used. The independant variable in this regression is the genetic effect of the gene represented by the genetic score. Covariates can be added in the model. In addition to the genetic scores CAST and WSS directly implemented in the package, the user can specify another genetic score for the regression.  
The variance-component test SKAT looks at the dispersion of genetics effects of rare variants. A geometrical interpretation of the test was used for its extension to more than two groups of individuals. Integration of covariates is also possible in this model.

###Genetic score for burden tests
We have implemented two functions to compute CAST and WSS scores respectively. These functions return a matrix with one row per individual and one column by genomic region. They are directly called in the function **burden.mlogit()** if these scores are used to perform the association tests. It is also possible to compute genetic scores in a gene based on a vector of weights for each variant using the function **burden.weighted.matrix()**.  

####CAST
CAST is based on a binary score which has a value of one if an individual carries at least one variant in the considered genomic region, and 0 otherwise. A MAF threshold for the definition of a rare variant is therefore needed as an argument to *maf.threshold*. This score can be computed using the function **CAST()** as shown here on the LCT data:
```{r}
#Calculation of the genetic score with a maf threshold of 1%
CAST.score <- CAST(x = x1, genomic.region = x1@snps$genomic.region, maf.threshold = 0.01)
head(CAST.score)
``` 

####WSS
WSS (Weighted Sum Statistic) is based on a continuous score giving the highest weights to the rarest variants: 
$$WSS_j = \sum_{i=1}^{R} I_{ij} * w_{i}$$ with $$w_{i} = \frac{1}{\sqrt(t_{i} * q_{i} * 1-q{i})}$$ and $$q_{i} = \frac{n_{i} + 1}{2*t_{i}+1}$$
Where $n_{i}$ is the total number of minor alleles genotyped for variant $i$, $t_{i}$ is the total number of alleles genotyped for variant $i$ and $I_{ij}$ is the number of minor alleles of variant $i$ for the invidual $j$. In the original method, each variant is weighted according to its frequency in the controls group. In our version of WSS, the weights depend on allele frequencies calculated on the entire sample. The function **WSS()** can be used to compute the WSS score as shown on the LCT data:
```{r}
WSS.score <- WSS(x = x1, genomic.region = x1@snps$genomic.region)
head(WSS.score)
``` 

####Other genetic scores
It is also possible to compute other genetic scores based on variants weights using the function **burden.weighted.matrix()**. The weights should be given as a vector to *weights* with the same size as the number of variants. The genetic score will be compute as well as:
$$Score_j = \sum_{i=1}^{R} I_{ij} * w_{i}$$
with $w_i$ the weight of each variant in *weights*, and $I_{ij}$ the number of minor alleles for individual $j$ in variant $i$.  
Here is an example corresponding to a genetic score with all the weights at 1, i.e. counting all the minor alleles:
```{r}
Sum.score <- burden.weighted.matrix(x = x1, weights = rep(1, ncol(x1)))
head(Sum.score)
```

#### Regressions
We have extended CAST and WSS using non-ordinal multinomial regression models. Let consider $C$ groups of individuals including a group of controls ($c=1$) and $C-1$ groups of cases with different sub-phenotypes of the disease. We can compute $C-1$ probability ratios, one for each group of cases: $$ln \frac{P(Y_{j}=c)}{P(Y_{j}=1)} = \beta_{0,c} + \beta_{G,c}X_{G} + \beta_{k1,c}K_{1}+...+\beta_{kl,c}K_{l}$$
Where $Y_{j}$ corresponds to the phenotype of the individual $j$ and $K_{l}$ is a vector for the $l$th covariate with the corresponding coefficient $\beta_{kl}$. The genetic effect is represented by $X_{G}$ and correspond to the genetic score CAST or WSS with $\beta_{G,c}$ the log-odds ratio associated to this burden score.  
The p-value associated to the genetic effect is calculated using a likelihood ratio test comparing this model to the same model without the genetic effect (null hypothesis). If only two groups are compared, a classical logistic regression is performed.  
This regression can be performed on a bed.matrix using the function **burden.mlogit()** which relies on the package mlogit. To do so, the user needs to specify a vector with the phenotype of each individual (argument *group*) and the gene associated to each variant (argument *genomic.region*).  
The  CAST or WSS genetic scores can be directly calculated in the regression (*burden="CAST"* or *burden="WSS"*). The user can also use another genetic score in the regression, which has to be specified as a matrix with one individual per row and one genomic region per column to *burden*.  In this situation, no bed matrix is needed, and the result from **burden.weighted.matrix()** can be used directly.  
The reference group of individuals should be given to the argument *ref.level*, i.e. all Odds Ratios will be computed in comparison to this group of individuals. The choice of the reference group won't affect the p-value.  
Potential covariates can also be included in the regression as a matrix with one row per individual and one column per covariate to the argument *data*. If only a subset of covariates from *data* are to be included in the model, a R formula should be given to *formula* with these covariates, otherwise all the covariates will be included.  
**burden.mlogit()** will return the p-value associated to the regression for each genomic region. If there is a convergence problem with the regression, the function will return 1 in the column *is.err*. The odds ratio associated to each group of cases compared to the reference group (*ref.level*) with its confidence interval at a given alpha threshold (argument *alpha*) can also be obtained if *get.OR.value=TRUE*.   
An example of the p-value and OR calculation with its 95% confidence interval using WSS on the LCT data is shown below with or without the inclusion of covariates. The outcome here corresponds to the population from 1000Genome.

```{r}

#WSS 
burden.mlogit(x=x1, group=x1@ped$pop, burden="WSS", ref.level="CEU", alpha=0.05, 
              get.OR.value=TRUE)

#Simulation of covariates with different probabilities in GBR/CEU/FIN and IBS/TSI
covar <- data.frame( sex = c(sample(0:1, sum(table(LCT.pop)[c("CEU", "GBR", "FIN")]), 
                                    TRUE, c(0.2,0.8)), 
                             sample(0:1, sum(table(LCT.pop)[c("TSI", "IBS")]), 
                                    TRUE, c(0.8,0.2))), 
                     u = runif(length(LCT.pop)))

#Regression with the covariate "sex" without OR values
burden.mlogit(x=x1, group=x1@ped$pop, burden="WSS", ref.level="CEU",
              data=covar, formula = ~ sex)

#WSS using directly the score matrix computed previously
burden.mlogit(burden=WSS.score, group=x1@ped$pop, ref.level="CEU")
```

###SKAT
We also extended the variance-component test SKAT using a geometric interpretation. Unlike the burden tests, the is no burden calculated in this test: the distribution of the genetic effects in the gene is compared to a null distribution. SKAT is based on a linear mixed model where the random effects correspond to the genetic effects.  
Permutations are used to compute the p-values with the arguments *perm.target* and *perm.max*. *perm.target* corresponds to the number of times a permutated statistics should be greater than the observed statistics, while *perm.max* corresponds to the maximum number of permutations to perform. A sequential procedure is used for these permutations: the program stops when any of these two values is reached.  
Two types of p-values estimations are then computed: if *perm.target* is reached, the p-value is computed as *perm.target* divided by the number of permutations performed to reach this value; if *perm.max* is reached before *perm.target* (that is, for pretty small p-values), the SKAT small sample procedure is used, and p-values are estimated using a chi-square distribution based on statistics moments obtained from the permutations.  
In this function, covariates can be included using the argument *Pi*. This arguments should be a matrix containing the probabilities that each individual belongs to each group, with one row per individul and one column per group of individuals. It can be directly computed using the function **Pi.matrix()** which has the same arguments *group*, *data*, *formula* and *ref.level* than **burden.mlogit()**. This function uses a regression with the *group* as the dependant variable and the covariates as the independant variables, and returns the adjusted probabilities. *ref.level* won't have any impact on the probability of each individual, but is needed to perform the regression.  
An example of this function and how to use it with SKAT is shown below.

```{r}
#Compute the Pi matrix with the covariate sex
Pi.matrix.LCT <- Pi.matrix(group=x1@ped$pop, data=covar, formula= ~ sex, ref.level="CEU")

#SKAT with the covariates
SKAT(x1, group=x1@ped$pop, Pi=Pi.matrix.LCT)
```


##Data management
Data in plink format or in vcf format can be loaded in R using the functions **read.bed.matrix()** and **read.vcf()** respectively from the package gaston.  
If the data for the controls and the different groups of cases are in different files, they can be loaded separately and then combined using the function **gaston:::rbind()** as long as the same variants are present between the different groups of individuals.  
An example is given below where the simulated data have been split according the the group of each individual, and then combined in a bed.matrix:

```{r}
#Selection of each group of individuals
CEU <- select.inds(x1, pop=="CEU")
CEU
FIN <- select.inds(x1, pop=="FIN")
FIN
GBR <- select.inds(x1, pop=="GBR")
GBR

#Combine in one file:
CEU.FIN.GBR <- rbind(CEU, FIN, GBR)
CEU.FIN.GBR
```


